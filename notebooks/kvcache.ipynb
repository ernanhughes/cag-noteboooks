{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from time import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\"\"\"Hugging Face Llama model\"\"\"\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "global model_name, model, tokenizer\n",
    "global rand_seed\n",
    "\n",
    "\n",
    "\n",
    "def generate(\n",
    "    model,\n",
    "    input_ids: torch.Tensor,\n",
    "    past_key_values,\n",
    "    max_new_tokens: int = 300\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate text with greedy decoding.\n",
    "\n",
    "    Args:\n",
    "        model: HuggingFace model with automatic device mapping\n",
    "        input_ids: Input token ids\n",
    "        past_key_values: KV Cache for knowledge\n",
    "        max_new_tokens: Maximum new tokens to generate\n",
    "    \"\"\"\n",
    "\n",
    "    embed_device = model.model.embed_tokens.weight.device\n",
    "\n",
    "    origin_ids = input_ids\n",
    "    input_ids = input_ids.to(embed_device)\n",
    "\n",
    "    output_ids = input_ids.clone()\n",
    "    next_token = input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(\n",
    "                input_ids=next_token, \n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = next_token_logits.argmax(dim=-1).unsqueeze(-1)\n",
    "            next_token = next_token.to(embed_device)\n",
    "\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            output_ids = torch.cat([output_ids, next_token], dim=1)\n",
    "\n",
    "            if next_token.item() in model.config.eos_token_id:\n",
    "                break\n",
    "    return output_ids[:, origin_ids.shape[-1]:]\n",
    "\n",
    "\n",
    "\"\"\"KV Cache test\"\"\"\n",
    "# Allowlist the DynamicCache class\n",
    "torch.serialization.add_safe_globals([DynamicCache])\n",
    "torch.serialization.add_safe_globals([set])\n",
    "\n",
    "\n",
    "def preprocess_knowledge(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    ") -> DynamicCache:\n",
    "    \"\"\"\n",
    "    Prepare knowledge kv cache for CAG.\n",
    "    Args:\n",
    "        model: HuggingFace model with automatic device mapping\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        prompt: The knowledge to preprocess, which is basically a prompt\n",
    "\n",
    "    Returns:\n",
    "        DynamicCache: KV Cache\n",
    "    \"\"\"\n",
    "    embed_device = model.model.embed_tokens.weight.device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(embed_device)\n",
    "    past_key_values = DynamicCache()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "    return outputs.past_key_values\n",
    "\n",
    "\n",
    "def write_kv_cache(kv: DynamicCache, path: str):\n",
    "    \"\"\"\n",
    "    Write the KV Cache to a file.\n",
    "    \"\"\"\n",
    "    torch.save(kv, path)\n",
    "\n",
    "\n",
    "def clean_up(kv: DynamicCache, origin_len: int):\n",
    "    \"\"\"\n",
    "    Truncate the KV Cache to the original length.\n",
    "    \"\"\"\n",
    "    for i in range(len(kv.key_cache)):\n",
    "        kv.key_cache[i] = kv.key_cache[i][:, :, :origin_len, :]\n",
    "        kv.value_cache[i] = kv.value_cache[i][:, :, :origin_len, :]\n",
    "\n",
    "\n",
    "def read_kv_cache(path: str) -> DynamicCache:\n",
    "    \"\"\"\n",
    "    Read the KV Cache from a file.\n",
    "    \"\"\"\n",
    "    kv = torch.load(path, weights_only=True)\n",
    "    return kv\n",
    "\n",
    "\n",
    "\"\"\"Sentence-BERT for evaluate semantic similarity\"\"\"\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Use a lightweight sentence-transformer\n",
    "\n",
    "def get_bert_similarity(response):\n",
    "    # Encode the query and text\n",
    "    query_embedding = bert_model.encode(response, convert_to_tensor=True)\n",
    "    text_embedding = bert_model.encode(ground_truth, convert_to_tensor=True)\n",
    "\n",
    "    # Compute the cosine similarity between the query and text\n",
    "    cosine_score = util.pytorch_cos_sim(query_embedding, text_embedding)\n",
    "\n",
    "    return cosine_score.item()\n",
    "\n",
    "\n",
    "def prepare_kv_cache(documents, filepath: str = \"./cache_knowledge.pt\", answer_instruction: str = None):\n",
    "    if answer_instruction is None:\n",
    "        answer_instruction = \"Answer the question with a super short answer.\"\n",
    "    knowledge = f\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are an assistant for giving short answers based on given context.<|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Context information is bellow.\n",
    "    ------------------------------------------------\n",
    "    {documents}\n",
    "    ------------------------------------------------\n",
    "    {answer_instruction}\n",
    "    Question:\n",
    "    \"\"\"\n",
    "    # Get the knowledge cache\n",
    "    t1 = time()\n",
    "    kv = preprocess_knowledge(model, tokenizer, knowledge)\n",
    "    print(\"KV Cache length: \", kv.key_cache[0].shape[-2])\n",
    "    write_kv_cache(kv, filepath)\n",
    "    t2 = time()\n",
    "    return kv, t2 - t1\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
