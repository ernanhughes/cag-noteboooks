{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from time import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache\n",
    "import random\n",
    "from python_dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\"\"\"Hugging Face Llama model\"\"\"\n",
    "HF_TOKEN = get_env()[\"HF_TOKEN\"]\n",
    "global model_name, model, tokenizer\n",
    "global rand_seed\n",
    "\n",
    "\n",
    "\n",
    "def generate(\n",
    "    model,\n",
    "    input_ids: torch.Tensor,\n",
    "    past_key_values,\n",
    "    max_new_tokens: int = 300\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate text with greedy decoding.\n",
    "\n",
    "    Args:\n",
    "        model: HuggingFace model with automatic device mapping\n",
    "        input_ids: Input token ids\n",
    "        past_key_values: KV Cache for knowledge\n",
    "        max_new_tokens: Maximum new tokens to generate\n",
    "    \"\"\"\n",
    "\n",
    "    embed_device = model.model.embed_tokens.weight.device\n",
    "\n",
    "    origin_ids = input_ids\n",
    "    input_ids = input_ids.to(embed_device)\n",
    "\n",
    "    output_ids = input_ids.clone()\n",
    "    next_token = input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(\n",
    "                input_ids=next_token, \n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = next_token_logits.argmax(dim=-1).unsqueeze(-1)\n",
    "            next_token = next_token.to(embed_device)\n",
    "\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            output_ids = torch.cat([output_ids, next_token], dim=1)\n",
    "\n",
    "            if next_token.item() in model.config.eos_token_id:\n",
    "                break\n",
    "    return output_ids[:, origin_ids.shape[-1]:]\n",
    "\n",
    "\n",
    "\"\"\"KV Cache test\"\"\"\n",
    "# Allowlist the DynamicCache class\n",
    "torch.serialization.add_safe_globals([DynamicCache])\n",
    "torch.serialization.add_safe_globals([set])\n",
    "\n",
    "\n",
    "def preprocess_knowledge(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    ") -> DynamicCache:\n",
    "    \"\"\"\n",
    "    Prepare knowledge kv cache for CAG.\n",
    "    Args:\n",
    "        model: HuggingFace model with automatic device mapping\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        prompt: The knowledge to preprocess, which is basically a prompt\n",
    "\n",
    "    Returns:\n",
    "        DynamicCache: KV Cache\n",
    "    \"\"\"\n",
    "    embed_device = model.model.embed_tokens.weight.device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(embed_device)\n",
    "    past_key_values = DynamicCache()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "    return outputs.past_key_values\n",
    "\n",
    "\n",
    "def write_kv_cache(kv: DynamicCache, path: str):\n",
    "    \"\"\"\n",
    "    Write the KV Cache to a file.\n",
    "    \"\"\"\n",
    "    torch.save(kv, path)\n",
    "\n",
    "\n",
    "def clean_up(kv: DynamicCache, origin_len: int):\n",
    "    \"\"\"\n",
    "    Truncate the KV Cache to the original length.\n",
    "    \"\"\"\n",
    "    for i in range(len(kv.key_cache)):\n",
    "        kv.key_cache[i] = kv.key_cache[i][:, :, :origin_len, :]\n",
    "        kv.value_cache[i] = kv.value_cache[i][:, :, :origin_len, :]\n",
    "\n",
    "\n",
    "def read_kv_cache(path: str) -> DynamicCache:\n",
    "    \"\"\"\n",
    "    Read the KV Cache from a file.\n",
    "    \"\"\"\n",
    "    kv = torch.load(path, weights_only=True)\n",
    "    return kv\n",
    "\n",
    "\n",
    "\"\"\"Sentence-BERT for evaluate semantic similarity\"\"\"\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Use a lightweight sentence-transformer\n",
    "\n",
    "def get_bert_similarity(response, ground_truth):\n",
    "    # Encode the query and text\n",
    "    query_embedding = bert_model.encode(response, convert_to_tensor=True)\n",
    "    text_embedding = bert_model.encode(ground_truth, convert_to_tensor=True)\n",
    "\n",
    "    # Compute the cosine similarity between the query and text\n",
    "    cosine_score = util.pytorch_cos_sim(query_embedding, text_embedding)\n",
    "\n",
    "    return cosine_score.item()\n",
    "\n",
    "\n",
    "def prepare_kvcache(documents, filepath: str = \"./data_cache/cache_knowledges.pt\", answer_instruction: str = None):\n",
    "    # Prepare the knowledges kvcache\n",
    "\n",
    "    if answer_instruction is None:\n",
    "        answer_instruction = \"Answer the question with a super short answer.\"\n",
    "    knowledges = f\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are an assistant for giving short answers based on given context.<|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Context information is bellow.\n",
    "    ------------------------------------------------\n",
    "    {documents}\n",
    "    ------------------------------------------------\n",
    "    {answer_instruction}\n",
    "    Question:\n",
    "    \"\"\"\n",
    "    # Get the knowledge cache\n",
    "    t1 = time()\n",
    "    kv = preprocess_knowledge(model, tokenizer, knowledges)\n",
    "    print(\"kvlen: \", kv.key_cache[0].shape[-2])\n",
    "    write_kv_cache(kv, filepath)\n",
    "    t2 = time()\n",
    "    return kv, t2 - t1\n",
    "\n",
    "\n",
    "def get_kis_dataset(filepath: str):\n",
    "    df = pd.read_csv(filepath)\n",
    "    dataset = zip(df['sample_question'], df['sample_ground_truth'])\n",
    "    text_list = df[\"ki_text\"].to_list()\n",
    "\n",
    "    return text_list, dataset\n",
    "\n",
    "\n",
    "def parse_squad_data(raw):\n",
    "    dataset = {\"ki_text\": [], \"qas\": []}\n",
    "\n",
    "    for k_id, data in enumerate(raw['data']):\n",
    "        article = []\n",
    "        for p_id, para in enumerate(data['paragraphs']):\n",
    "            article.append(para['context'])\n",
    "            for qa in para['qas']:\n",
    "                ques = qa['question']\n",
    "                answers = [ans['text'] for ans in qa['answers']]\n",
    "                dataset['qas'].append({\"title\": data['title'], \"paragraph_index\": tuple((k_id, p_id)), \"question\": ques, \"answers\": answers})\n",
    "        dataset['ki_text'].append({\"id\": k_id, \"title\": data['title'], \"paragraphs\": article})\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_squad_dataset(filepath: str, max_knowledge: int = None,\n",
    "                      max_paragraph: int = None, max_questions: int = None):\n",
    "    # Open and read the JSON file\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    # Parse the SQuAD data\n",
    "    parsed_data = parse_squad_data(data)\n",
    "\n",
    "    print(\"max_knowledge\", max_knowledge, \"max_paragraph\", max_paragraph, \"max_questions\", max_questions)\n",
    "\n",
    "    # Set the limit Maximum Articles, use all Articles if max_knowledge is None or greater than the number of Articles\n",
    "    max_knowledge = max_knowledge if max_knowledge is not None and max_knowledge < len(parsed_data['ki_text']) else len(parsed_data['ki_text'])\n",
    "\n",
    "    # Shuffle the Articles and Questions\n",
    "    if rand_seed is not None:\n",
    "        random.seed(rand_seed)\n",
    "        random.shuffle(parsed_data[\"ki_text\"])\n",
    "        random.shuffle(parsed_data[\"qas\"])\n",
    "        k_ids = [i['id'] for i in parsed_data[\"ki_text\"][:max_knowledge]]\n",
    "\n",
    "    text_list = []\n",
    "    # Get the knowledge Articles for at most max_knowledge, or all Articles if max_knowledge is None\n",
    "    for article in parsed_data['ki_text'][:max_knowledge]:\n",
    "        max_para = max_paragraph if max_paragraph is not None and max_paragraph < len(article['paragraphs']) else len(article['paragraphs'])\n",
    "        text_list.append(article['title'])\n",
    "        text_list.append('\\n'.join(article['paragraphs'][0:max_para]))\n",
    "\n",
    "    # Check if the knowledge id of qas is less than the max_knowledge\n",
    "    questions = [qa['question'] for qa in parsed_data['qas'] if qa['paragraph_index'][0] in k_ids and (max_paragraph is None or qa['paragraph_index'][1] < max_paragraph)]\n",
    "    answers = [qa['answers'][0] for qa in parsed_data['qas'] if qa['paragraph_index'][0] in k_ids and (max_paragraph is None or qa['paragraph_index'][1] < max_paragraph)]\n",
    "\n",
    "    dataset = zip(questions, answers)\n",
    "\n",
    "    return text_list, dataset\n",
    "\n",
    "\n",
    "def get_hotpotqa_dataset(filepath: str, max_knowledge: int = None):\n",
    "    # Open and read the JSON\n",
    "    with open(filepath, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    if rand_seed is not None:\n",
    "        random.seed(rand_seed)\n",
    "        random.shuffle(data)\n",
    "\n",
    "    questions = [qa['question'] for qa in data]\n",
    "    answers = [qa['answer'] for qa in data]\n",
    "    dataset = zip(questions, answers)\n",
    "\n",
    "    if max_knowledge is None:\n",
    "        max_knowledge = len(data)\n",
    "    else:\n",
    "        max_knowledge = min(max_knowledge, len(data))\n",
    "\n",
    "    text_list = []\n",
    "    for i, qa in enumerate(data[:max_knowledge]):\n",
    "        context = qa['context']\n",
    "        context = [c[0] + \": \\n\" + \"\".join(c[1]) for c in context]\n",
    "        article = \"\\n\\n\".join(context)\n",
    "\n",
    "        text_list.append(article)\n",
    "\n",
    "    return text_list, dataset\n",
    "\n",
    "\n",
    "        if args.usePrompt:\n",
    "            prompt = f\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are an assistant for giving short answers based on given context.<|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Context information is bellow.\n",
    "    ------------------------------------------------\n",
    "    {knowledges}\n",
    "    ------------------------------------------------\n",
    "    {answer_instruction}\n",
    "    Question:\n",
    "    {question}<|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "            generate_t1 = time()\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            output = generate(model, input_ids, DynamicCache()) \n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True, temperature=None)\n",
    "            generate_t2 = time()\n",
    "        else:\n",
    "            prompt = f\"\"\"\n",
    "    {question}<|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "            generate_t1 = time()\n",
    "            clean_up(knowledge_cache, kv_len)\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            output = generate(model, input_ids, knowledge_cache)\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True, temperature=None)\n",
    "            generate_t2 = time()\n",
    "\n",
    "        # print(\"D: \", knowledges)\n",
    "        print(\"Q: \", question)\n",
    "        print(\"A: \", generated_text)\n",
    " \n",
    "        # Evaluate bert-score similarity\n",
    "        similarity = get_bert_similarity(generated_text, ground_truth)\n",
    "\n",
    "        print(f\"[{id}]: Semantic Similarity: {round(similarity, 5)},\",\n",
    "              f\"cache time: {cache_t2 - cache_t1},\",\n",
    "              f\"generate time: {generate_t2 - generate_t1}\")\n",
    "        with open(args.output, \"a\") as f:\n",
    "            f.write(f\"[{id}]: Semantic Similarity: {round(similarity, 5)},\\t cache time: {cache_t2 - cache_t1},\\t generate time: {generate_t2 - generate_t1}\\n\")\n",
    "\n",
    "        results[\"prompts\"].append(question)\n",
    "        results[\"responses\"].append(generated_text)\n",
    "        results[\"cache_time\"].append(cache_t2 - cache_t1)\n",
    "        results[\"generate_time\"].append(generate_t2 - generate_t1)\n",
    "        results[\"similarity\"].append(similarity)\n",
    "\n",
    "        with open(args.output, \"a\") as f:\n",
    "            f.write(f\"[{id}]: [Cumulative]: \"\n",
    "                    + f\"Semantic Similarity: {round(sum(results['similarity']) / (len(results['similarity'])) , 5)},\"\n",
    "                    + f\"\\t cache time: {sum(results['cache_time']) / (len(results['cache_time'])) },\"\n",
    "                    + f\"\\t generate time: {sum(results['generate_time']) / (len(results['generate_time'])) }\\n\")\n",
    "\n",
    "    avg_similarity = sum(results[\"similarity\"]) / len(results[\"similarity\"])\n",
    "    avg_cache_time = sum(results[\"cache_time\"]) / len(results[\"cache_time\"])\n",
    "    avg_generate_time = sum(results[\"generate_time\"]) / len(results[\"generate_time\"])\n",
    "    print()\n",
    "    print(f\"Prepare time: {prepare_time}\")\n",
    "    print(f\"Average Semantic Similarity: {avg_similarity}\")\n",
    "    print(f\"cache time: {avg_cache_time},\\t generate time: {avg_generate_time}\")\n",
    "    print()\n",
    "    with open(args.output, \"a\") as f:\n",
    "        f.write(\"\\n\")\n",
    "        f.write(f\"Result for {args.output}\\n\")\n",
    "        f.write(f\"Prepare time: {prepare_time}\\n\")\n",
    "        f.write(f\"Average Semantic Similarity: {avg_similarity}\\n\")\n",
    "        f.write(f\"cache time: {avg_cache_time},\\t generate time: {avg_generate_time}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def load_quantized_model(model_name, hf_token=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",          # Automatically choose best device\n",
    "        trust_remote_code=True,     # Required for some models\n",
    "        token=hf_token\n",
    "    )\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
